---
title: "#NICAR18 Tweets"
output: 
  github_document:
    toc: true
    df_print: "kable"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE, comment = "#>")
```

This is a dedicated repository for tracking [#NICAR18 tweets](https://twitter.com/hashtag/NICAR18?f=tweets&vertical=default&src=hash) (the official hashtag of 2018 annual Computer-Assisted Reporting Conference).

## Data

### rtweet

Whether you lookup the status IDs or search/stream new tweets, you'll need to make sure to install the [rtweet](http://rtweet.info) package. The code below will install [if it's not already] and load rtweet.

```{r}
## install rtweet if not already
if (!requireNamespace("rtweet", quietly = TRUE)) {
  install.packages("rtweet")
}

## load rtweet
library(rtweet)
```

Our data collection method is described in detail below. However, if you want to get straight to the data, simply run the following code:

```{r, eval=FALSE}
## download status IDs file
download.file(
  "https://github.com/computer-assisted-reporting/NICAR18/blob/master/data/search-ids.rds?raw=true",
  "NICAR18_status_ids.rds"
)

## read status IDs fromdownloaded file
ids <- readRDS("NICAR18_status_ids.rds")

## lookup data associated with status ids
rt <- rtweet::lookup_tweets(ids$status_id)
```


### Search

One of the easiest ways to gather Twitter data is to search for the data (using Twitter's REST API). Unlike streaming, searching makes it possible to go back in time. Unfortunately, Twitter sets a rather restrictive cap–roughly nine days–on how far back you can go. Regardless, searching for tweets is often the preferred method. For example, the code below is setup in such a way that it can be executed once [or even several times] a day throughout the conference. See the [R code here](search.R).

Here's some example code showing what essentially we're doing to collect the data:

```{r, echo=FALSE}
source("search.R")
```

```{r, eval=FALSE}
## search terms
nicar18conf <- c("NICAR18", "NICAR2018", "IRE_NICAR")

## search for up to 10,000 tweets mentioning nicar18
rt <- search_tweets(paste(nicar18conf, collapse = " OR "), n = 10000)
```

## Explore

To explore the Twitter data, we recommend using the [tidyverse](http://tidyverse.org) packages. We're also using a customized [ggplot2](http://ggplot2.org) theme. See the [R code here](tidyggplot.R).

```{r, echo=FALSE}
source("tidyggplot.R")
```

### Tweet frequency over time

To create the image below, the data were summarized into a time series-like data frame and then plotted in order depict the frequency of tweets–aggregated in two-hour intevals–about \#nicar18 over time. See the [R code here](ts.R).

```{r timefreq, echo=FALSE}
source("ts.R")
```

<p align="center"><img width="100%" height="auto" src="img/timefreq.png" /></p>

&nbsp;

### Positive/negative sentiment

Next, some sentiment analysis of the tweets so far. See the [R code here](sentiment.R).

```{r sentiment, echo=FALSE}
source("sentiment.R")
```

<p align="center"><img width="100%" height="auto" src="img/sentiment.png" /></p>

&nbsp;

### Semantic networks

The image below depicts a quick and dirty visualization of the semantic network (connections via retweet, quote, mention, or reply) as it is observed in the data. See the [R code here](network.R).

```{r network, echo=FALSE}
source("network.R")
```


<p align="center"><img width="100%" height="auto" src="img/network.png" /></p>

&nbsp;

Ideally, the network visualization would be an interactive, searchable graphic. Since it's not, I've printed out the node size values below.

```{r}
nodes <- as_tibble(sort(size, decreasing = TRUE))
nodes$rank <- seq_len(nrow(nodes))
nodes$screen_name <- paste0(
  '<a href="https://twitter.com/', nodes$screen_name, 
  '">@', nodes$screen_name, '</a>')
nodes$n <- round(nodes$n, 3)
dplyr::select(nodes, rank, screen_name, log_n = n)
```
